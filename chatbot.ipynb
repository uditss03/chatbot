{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bSVl0mJ6VN6",
        "colab_type": "code",
        "outputId": "aba5dd04-295b-4daf-bfda-f606e71a3188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import Input,Dense,Bidirectional,GRU,LSTM,Embedding,Dropout,Flatten\n",
        "from keras.layers.convolutional import Convolution2D,MaxPooling2D,ZeroPadding2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint, TensorBoard  \n",
        "from keras import backend as K  \n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLNRpJIrHQ29",
        "colab_type": "text"
      },
      "source": [
        "**DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5Xkd3MT66Zr",
        "colab_type": "code",
        "outputId": "40ff1210-cfa0-45b5-e931-a4d2a7e44f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "x=df['x']\n",
        "y = df['y']\n",
        "\n",
        "print(\"sample input  :\",x[35])\n",
        "print(\"sample output :\", y[35])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample input  : Me.  This endless ...blonde babble. I'm like, boring myself.\n",
            "sample output : Thank God!  If I had to hear one more story about your coiffure...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CATWePq1E0hb",
        "colab_type": "code",
        "outputId": "0eb28716-5ab0-4edb-85c2-5ef74b21f122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mark_start = 'ssss '\n",
        "mark_end = ' eeee'\n",
        "def mark_captions(captions_list):\n",
        "    captions_marked = [mark_start + caption + mark_end for caption in captions_list]\n",
        "    return captions_marked\n",
        "\n",
        "y = mark_captions(y)\n",
        "print(\"input : \",x[35])\n",
        "print(\"output : \",y[35])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input :  Me.  This endless ...blonde babble. I'm like, boring myself.\n",
            "output :  ssss Thank God!  If I had to hear one more story about your coiffure... eeee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvKHczalKGZx",
        "colab_type": "code",
        "outputId": "40a94fee-f2e7-4c12-fefd-19c26c5d2004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "def reverseSen(Sentence):\n",
        "  words = Sentence.split(\" \") \n",
        "  newWords = [words[i] for i in range(len(words)-1,-1,-1)] \n",
        "  newSentence = \" \".join(newWords) \n",
        "  return newSentence\n",
        "input_x = [reverseSen(each) for each in x]\n",
        "#input_x = np.array(input_x)\n",
        "output_y = y\n",
        "#output_y = np.array(output_y)\n",
        "print(\"reversing input : \",input_x[35])\n",
        "print(\"input shape : \",len(input_x))\n",
        "print(\"output : \",output_y[35])\n",
        "print(\"output shape : \",len(output_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reversing input :  myself. boring like, I'm babble. ...blonde endless This  Me.\n",
            "input shape :  221282\n",
            "output :  ssss Thank God!  If I had to hear one more story about your coiffure... eeee\n",
            "output shape :  221282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caQZM9I-CrIC",
        "colab_type": "code",
        "outputId": "c6e9251d-b767-43df-bf84-16d928e45cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "data_text = input_x+output_y\n",
        "num_words = 10000\n",
        "tokenizer = Tokenizer(num_words = num_words)\n",
        "tokenizer.fit_on_texts(data_text)\n",
        "\n",
        "\n",
        "x_tokens = tokenizer.texts_to_sequences(input_x)\n",
        "y_tokens = tokenizer.texts_to_sequences(output_y)\n",
        "\n",
        "\n",
        "print(\"tokenized input : \" , x_tokens[35])\n",
        "print(\"tokenized output : \", y_tokens[35])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized input :  [308, 1796, 38, 20, 2294, 7117, 17, 14]\n",
            "tokenized output :  [2, 235, 196, 43, 4, 99, 6, 241, 56, 113, 389, 39, 23, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZkZcq2XOPia",
        "colab_type": "code",
        "outputId": "ab5d9ad0-8d16-4db6-d457-d5d9935d51a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lens = [len(each) for each in x_tokens+y_tokens]\n",
        "lens = np.array(lens)\n",
        "\n",
        "max_tokens = np.mean(lens) + 2*np.std(lens)\n",
        "max_tokens = int(max_tokens)\n",
        "print(max_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9D5eD0VCw32",
        "colab_type": "code",
        "outputId": "0b279f73-a350-4d31-bdc1-e51d59347983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "pad = 'pre'\n",
        "x_pad = pad_sequences(x_tokens,maxlen = max_tokens,padding = pad,truncating = pad)\n",
        "pad = 'post'\n",
        "y_pad = pad_sequences(y_tokens,maxlen = max_tokens,padding = pad,truncating = pad)\n",
        "\n",
        "encoder_in_data = x_pad\n",
        "decoder_in_data = y_pad\n",
        "decoder_output_data = y_pad[:,1:]\n",
        "\n",
        "decoder_out_data = [np.insert(each,len(each),0) for each in decoder_output_data]\n",
        "\n",
        "\n",
        "print(\"padded encoder input : \",encoder_in_data[35])\n",
        "print(\"padded decoder input : \",decoder_in_data[35])\n",
        "print(\"padded decoder output : \",decoder_out_data[35])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "padded encoder input :  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0  308 1796   38\n",
            "   20 2294 7117   17   14]\n",
            "padded decoder input :  [  2 235 196  43   4  99   6 241  56 113 389  39  23   1   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "padded decoder output :  [235 196  43   4  99   6 241  56 113 389  39  23   1   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt6PKgF3Dimj",
        "colab_type": "code",
        "outputId": "54cb82f5-0be1-4a3b-c158-e1ad4fc9f2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for i in range(0,len(decoder_in_data)):\n",
        "  for j in range(0,len(decoder_in_data[i])):\n",
        "    if( decoder_in_data[i][j] == 1):\n",
        "      decoder_in_data[i][j] = 0\n",
        "decoder_in_data[35]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  2, 235, 196,  43,   4,  99,   6, 241,  56, 113, 389,  39,  23,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8nu7yyUDte-",
        "colab_type": "code",
        "outputId": "c0540246-6ee4-4fa1-d647-53dedbac3c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "encoder_in_data = np.array(encoder_in_data)\n",
        "decoder_in_data = np.array(decoder_in_data)\n",
        "decoder_out_data = np.array(decoder_out_data)\n",
        "print(\"encoder input shape\",encoder_in_data.shape)\n",
        "print(\"decoder input shape\",decoder_in_data.shape)\n",
        "print(\"decoder output shape\",decoder_out_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder input shape (221282, 33)\n",
            "decoder input shape (221282, 33)\n",
            "decoder output shape (221282, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfWU_5kbDzUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = \\\n",
        "{\n",
        "    'encoder_input': encoder_in_data,\n",
        "    'decoder_input': decoder_in_data\n",
        "}\n",
        "\n",
        "y_data = \\\n",
        "{\n",
        "    'decoder_output': decoder_out_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PEcj7qzSHm_",
        "colab_type": "text"
      },
      "source": [
        "**ENCODER DECODER MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2YgZ4oBS9GU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = Input(shape=(None, ), name='encoder_input')\n",
        "embedding_size = 128\n",
        "encoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='encoder_embedding')\n",
        "state_size = 512\n",
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
        "                   return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
        "                   return_sequences=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
        "                   return_sequences=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77h3Tj9zUP1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_encoder():\n",
        "    # Start the neural network with its input-layer.\n",
        "    net = encoder_input\n",
        "    \n",
        "    # Connect the embedding-layer.\n",
        "    net = encoder_embedding(net)\n",
        "\n",
        "    # Connect all the GRU-layers.\n",
        "    net = encoder_gru1(net)\n",
        "    net = encoder_gru2(net)\n",
        "    net = encoder_gru3(net)\n",
        "\n",
        "    # This is the output of the encoder.\n",
        "    encoder_output = net\n",
        "    \n",
        "    return encoder_output\n",
        "encoder_output = connect_encoder()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_nporJWQl4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                              name='decoder_initial_state')\n",
        "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
        "decoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='decoder_embedding')\n",
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)\n",
        "decoder_dense = Dense(num_words,\n",
        "                      activation='linear',\n",
        "                      name='decoder_output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lylvx32rOwOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def connect_decoder(initial_state):\n",
        "    # Start the decoder-network with its input-layer.\n",
        "    net = decoder_input\n",
        "\n",
        "    # Connect the embedding-layer.\n",
        "    net = decoder_embedding(net)\n",
        "    \n",
        "    # Connect all the GRU-layers.\n",
        "    net = decoder_gru1(net, initial_state=initial_state)\n",
        "    net = decoder_gru2(net, initial_state=initial_state)\n",
        "    net = decoder_gru3(net, initial_state=initial_state)\n",
        "\n",
        "    # Connect the final dense layer that converts to\n",
        "    # one-hot encoded arrays.\n",
        "    decoder_output = decoder_dense(net)\n",
        "    \n",
        "    return decoder_output\n",
        "decoder_output = connect_decoder(initial_state=encoder_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zXJTgB5O2LG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train = Model(inputs=[encoder_input, decoder_input],\n",
        "                    outputs=[decoder_output])\n",
        "model_encoder = Model(inputs=[encoder_input],\n",
        "                      outputs=[encoder_output])\n",
        "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
        "\n",
        "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
        "                      outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awZDKghOPDtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "def sparse_cross_entropy(y_true, y_pred):  \n",
        "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
        "    loss_mean = tf.reduce_mean(loss)\n",
        "    return loss_mean\n",
        "\n",
        "optimizer = RMSprop(lr=1e-3)\n",
        "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk8Fu_58PF34",
        "colab_type": "code",
        "outputId": "90fd9777-813e-49ea-da54-302081dd7179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "model_train.compile(optimizer=optimizer,\n",
        "                    loss=sparse_cross_entropy,\n",
        "                    target_tensors=[decoder_target])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0821 10:36:41.382435 139885559637888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I80bIUWyuucZ",
        "colab_type": "code",
        "outputId": "34a81dea-d9af-4a12-8df3-2268fc08dd8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "model_train.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_embedding (Embedding)   (None, None, 128)    1280000     encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_gru1 (GRU)              (None, None, 512)    984576      encoder_embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_gru2 (GRU)              (None, None, 512)    1574400     encoder_gru1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_embedding (Embedding)   (None, None, 128)    1280000     decoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder_gru3 (GRU)              (None, 512)          1574400     encoder_gru2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_gru1 (GRU)              (None, None, 512)    984576      decoder_embedding[0][0]          \n",
            "                                                                 encoder_gru3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_gru2 (GRU)              (None, None, 512)    1574400     decoder_gru1[0][0]               \n",
            "                                                                 encoder_gru3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_gru3 (GRU)              (None, None, 512)    1574400     decoder_gru2[0][0]               \n",
            "                                                                 encoder_gru3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder_output (Dense)          (None, None, 10000)  5130000     decoder_gru3[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 15,956,752\n",
            "Trainable params: 15,956,752\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58l7zgFvPcrJ",
        "colab_type": "code",
        "outputId": "05558f07-8868-40a4-b5c9-863fada5c003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=512,\n",
        "                epochs=15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            " 11776/221282 [>.............................] - ETA: 8:31 - loss: 1.2981"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEc4ZTtoS4Q9",
        "colab_type": "code",
        "outputId": "ade97a53-f110-470f-ab7a-7c05261cbe61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "model_encoder.save('e.h5')\n",
        "model_decoder.save('d.h5')\n",
        "\n",
        "files.download('e.h5')\n",
        "files.download('d.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_gru1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_initial_state_1:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_gru2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_initial_state_1:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n",
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer decoder_gru3 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_initial_state_1:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2336aef5e0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'e.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'd.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FDdrk7wPqzm",
        "colab_type": "code",
        "outputId": "3f8b73fe-9c2d-4473-c376-09e3474896b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def reply(test):\n",
        "    test = [test]\n",
        "    test_tokens = tokenizer.texts_to_sequences(test)\n",
        "    pad = 'pre'\n",
        "    test_pad = pad_sequences(test_tokens,maxlen = max_tokens,padding = pad,truncating = pad)\n",
        "    initial_state = model_encoder.predict(test_pad)\n",
        "    shape = (1, max_tokens)\n",
        "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
        "    token_int = 2\n",
        "    token_end = 1\n",
        "    output = []\n",
        "    count_tokens =0    \n",
        "    while token_int != token_end and count_tokens<max_tokens:\n",
        "        decoder_input_data[0,count_tokens] = token_int\n",
        "        test_data = {'decoder_initial_state': initial_state,'decoder_input': decoder_input_data}\n",
        "        decoder_output = model_decoder.predict(test_data)\n",
        "        token_onehot = decoder_output[0, count_tokens, :]\n",
        "        token_int = np.argmax(token_onehot)\n",
        "        output.append(token_int)\n",
        "        count_tokens+=1\n",
        "    for i in range(0,len(output)-1):\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if(index == output[i]) :\n",
        "                print(word,end = ' ')\n",
        "    print()\n",
        "\n",
        "i=0\n",
        "while(True):\n",
        "    t = input('human: ')\n",
        "    reply(t)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "human: Hi life is good\n",
            "good night \n",
            "human: good morning\n",
            "good night \n",
            "human: what going on\n",
            "i don't know \n",
            "human: i love you\n",
            "i love you \n",
            "human: m going for the movie tomorrow\n",
            "what \n",
            "human: yeah its cool right\n",
            "what do you mean \n",
            "human: are you human\n",
            "yes \n",
            "human: your name\n",
            "i don't know \n",
            "human: what you dont know\n",
            "i don't know \n",
            "human: its gonna be alright\n",
            "i don't know what to do \n",
            "human: you must know \n",
            "no \n",
            "human: you should know\n",
            "i don't know \n",
            "human: i like sanchita mittal\n",
            "you know i don't know what you mean \n",
            "human: fuck off\n",
            "what \n",
            "human: i said fuck off asshole\n",
            "you don't have to do it \n",
            "human: i will do it right nwo\n",
            "i don't think so \n",
            "human: i am telling you\n",
            "what \n",
            "human: yeah be prepared\n",
            "yeah \n",
            "human: mmm your attitude\n",
            "i don't know what to say \n",
            "human: fuck off bitch\n",
            "what \n",
            "human: colour you like most\n",
            "yeah \n",
            "human: i like blue\n",
            "and you have to tell me \n",
            "human: its okay sharing secret\n",
            "i don't know \n",
            "human: tell me about you\n",
            "i don't know \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7867dc5ba8ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'human: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-38_tG_Zb4pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}